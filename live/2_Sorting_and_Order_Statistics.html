<html>
<head>
  <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>
</head>
<body>
  <h1>II Sorting and Order Statistics</h1>
  <h2>Chapter 6 Heapsort</h2>
  <h2>Chapter 7 Quicksort</h2>
  <h3>7.1 Description of quicksort</h3>
  
  
  <a href="insertion_sort.m">insertion_sort.m</a>
  
  <p><strong>Exercise 7.1-1</strong></p>
  <p>Using Figure 7.1 as a model, illustrate the operation of \(\text{Partition}\) on the array \(A=\langle 13,19,9,5,12,8,7,4,21,2,6,11\rangle\).</p>
  <p><em>Answer:</em></p>
\[
\begin{array}{}
13 & 19 & 9 & 5 & 12 & 8 & 7 & 4 & 21 & 2 & 6 & 11\\
13 & 19 & 9 & 5 & 12 & 8 & 7 & 4 & 21 & 2 & 6 & 11\\
13 & 19 & 9 & 5 & 12 & 8 & 7 & 4 & 21 & 2 & 6 & 11\\
9 & 19 & 13 & 5 & 12 & 8 & 7 & 4 & 21 & 2 & 6 & 11\\
9 & 5 & 13 & 19 & 12 & 8 & 7 & 4 & 21 & 2 & 6 & 11\\
9 & 5 & 13 & 19 & 12 & 8 & 7 & 4 & 21 & 2 & 6 & 11\\
9 & 5 & 8 & 19 & 12 & 13 & 7 & 4 & 21 & 2 & 6 & 11\\
9 & 5 & 8 & 7 & 12 & 13 & 19 & 4 & 21 & 2 & 6 & 11\\
9 & 5 & 8 & 7 & 4 & 13 & 19 & 12 & 21 & 2 & 6 & 11\\
9 & 5 & 8 & 7 & 4 & 13 & 19 & 12 & 21 & 2 & 6 & 11\\
9 & 5 & 8 & 7 & 4 & 2 & 19 & 12 & 21 & 13 & 6 & 11\\
9 & 5 & 8 & 7 & 4 & 2 & 6 & 12 & 21 & 13 & 19 & 11\\
9 & 5 & 8 & 7 & 4 & 2 & 6 & 11 & 21 & 13 & 19 & 12
\end{array}
\]
  <p><strong>Exercise 7.1-2</strong></p>
  <p>What value of \(q\) does \(\text{Partition}\) return when all elements in the array \(A[p..r]\) have the same value? Modify \(\text{Partition}\) so that \(q=\lfloor(p+r)/2\rfloor\) when all elements in the array \(A[p..r]\) have the same value.</p>
  <p><em>Answer:</em></p>
\[r\]
  <p><strong>Exercise 7.1-3</strong></p>
  <p>Give a brief argument that the running time of \(\text{Partition}\) on a subarray of size \(n\) is \(\Theta(n)\).</p>
  <p><strong>Exercise 7.1-4</strong></p>
  <p>How would you modify \(\text{Quicksort}\) to sort into nonincreasing order?</p>
  <h3>7.2 Performance of quicksort</h3>
  <p><strong>Exercise 7.2-1</strong></p>
  <p>Use the substitution method to prove that the recurrence \(T(n)=T(n-1)+\Theta(n)\) has the solution \(T(n)=\Theta(n^2)\), as claimed at the beginning of Section 7.2.</p>
  <p><em>Answer:</em></p>
\[
\begin{alignat*}{4}
& &T(n)&=T(n-1)+\Theta(n)\\
c_1(n-1)^2+c_2n &\le&\ T(n)&\le c_3(n-1)^2+c_4n\\
c_1n^2+(c_2-2c_1)n+c_1 &\le&\ T(n)&\le c_3n^2+(c_4-2c_3)n+c_3\\
c_1n^2 &\le&\ T(n)&\le c_3n^2 \qquad(c_1\lt \frac {c_2}2,c_3\gt \frac {c_4}2,n \ge \frac {c_3}{2c_3-c_4})
\end{alignat*}
\]
  <p><strong>Exercise 7.2-2</strong></p>
  <p>What is the running time of \(\text{Quicksort}\) when all elements of array \(A\) have the same value?</p>
  <p><em>Answer:</em></p>
\[\Theta(n^2)\]
  <p><strong>Exercise 7.2-3</strong></p>
  <p>Show that the running time of \(\text{Quicksort}\) is \(\Theta(n^2)\) when the array A contains distinct elements and is sorted in decreasing order.</p>
  <p><strong>Exercise 7.2-4</strong></p>
  <p>Banks often record transactions on an account in order of the times of the transactions, but many people like to receive their bank statements with checks listed in order by check number. People usually write checks in order by check number, and merchants usually cash them with reasonable dispatch. The problem of converting time-of-transaction ordering to check-number ordering is therefore the problem of sorting almost-sorted input. Argue that the procedure \(\text{InsertionSort}\) would tend to beat the procedure \(\text{Quicksort}\) on this problem.</p>
  <p><strong>Exercise 7.2-5</strong></p>
  <p>Suppose that the splits at every level of quicksort are in the proportion \(1-\alpha\) to \(\alpha\), where \(0\lt\alpha \le 1/2\) is a constant. Show that the minimum depth of a leaf in the recursion tree is approximately \(-\lg n/\lg\alpha\) and the maximum depth is approximately \(-\lg n/\lg(1-\alpha)\). (Don't worry about integer round-off.)</p>
  <p><em>Answer:</em></p>
\[
\begin{align*}
n \left(\frac 1 \alpha\right)^{h_1}&=1\\
h_1&=-\frac {\lg n}{\lg\alpha}\\
\\
n \left(\frac 1 {1-\alpha}\right)^{h_2}&=1\\
h_2&=-\frac {\lg n}{\lg(1-\alpha)}\\
\end{align*}
\]
  <p><strong>Exercise 7.2-6 &#9733;</strong></p>
  <p>Argue that for any constant \(0\lt\alpha \le 1/2\), the probability is approximately \(1-2\alpha\) that on a random input array, \(\text{Partition}\) produces a split more balanced than \(1-\alpha\) to \(\alpha\).</p>
  <h3>7.3 A randomized version of quicksort</h3>
  <p><strong>Exercise 7.3-1</strong></p>
  <p>Why do we analyze the expected running time of a randomized algorithm and not its worst-case running time?</p>
  <p><strong>Exercise 7.3-2</strong></p>
  <p>When \(\text{RandomizedQuicksort}\) runs, how many calls are made to the random-number generator \(\text{Random}\) in the worst case? How about in the best case? Give your answer in terms of \(\Theta\)-notation.</p>
  <p><em>Answer:</em></p>
\[\Theta(n)\]
  <h3>7.4 Analysis of quicksort</h3>
  <p><strong>Exercise 7.4-1</strong></p>
  <p>Show that in the recurrence</p>
  <p>\(T(n)=\max\limits_{0\le q\le n-1}(T(q)+T(n-q-1))+\Theta(n)\),</p>
  <p>\(T(n)=\Omega(n^2)\).</p>
  <p><em>Answer:</em></p>
\[
\begin{align*}
T(n)&\ge \max_{0\le q\le n-1}(cq^2+c(n-q-1)^2)+\Theta(n)\\
&= c\cdot\max_{0\le q\le n-1}(q^2+(n-q-1)^2)+\Theta(n)\\
&= cn^2-c(2n-1)+\Theta(n)\\
&\ge cn^2\\
\end{align*}
\]
  <p><strong>Exercise 7.4-2</strong></p>
  <p>Show that quicksort's best-case running time is \(\Omega(n\lg n)\).</p>
  <p><em>Answer:</em></p>
\[
\begin{align*}
T(n)&=\min_{0\le q\le n-1}(T(q)+T(n-q-1))+\Theta(n)\\
&\ge \min_{0\le q\le n-1}(cq\lg q+c(n-q-1)\lg (n-q-1))+\Theta(n)\\
&= c\cdot\min_{0\le q\le n-1}(q\lg q+(n-q-1)\lg (n-q-1))+\Theta(n)\\
&= c(n-1)\lg{\frac{n-1}2}+\Theta(n)\\
&\gt c(n-1)\lg{\frac n4}+\Theta(n)\qquad(n \gt 2)\\
&= cn\lg n-c(2n+\lg n-2)+\Theta(n)\\
&\ge cn\lg n\\
\end{align*}
\]
  <p><strong>Exercise 7.4-3</strong></p>
  <p>Show that the expression \(q^2+(n-q-1)^2\) achieves a maximum over \(q=0,1,\dots,n-1\) when \(q=0\) or \(q=n-1\).</p>
  <p><em>Answer:</em></p>
\[
\begin{align*}
f(q)&=q^2+(n-q-1)^2\\
f'(q)&=4q-2n+2\\
f'(q)&\lt 0, \ q\lt \frac {n-1}2\\
f'(q)&\gt 0, \ q\gt \frac {n-1}2\\
\end{align*}
\]
  <p><strong>Exercise 7.4-4</strong></p>
  <p>Show that \(\text{RandomizedQuicksort}\)'s expected running time is \(\Omega(n\lg n)\).</p>
  <p><em>Answer:</em></p>
\[
\begin{align*}
\text E[X]&=\sum_{i=1}^{n-1}\sum_{k=1}^{n-i}\frac 2{k+1}\\
&\ge \sum_{i=1}^{n-1}\sum_{k=1}^{n-i}\frac 2{k+k}\\
&= \sum_{i=1}^{n-1}\sum_{k=1}^{n-i}\frac 1k\\
&\gt \sum_{i=1}^{n-1}\ln(n-i)\\
&= \ln ((n-1)!)\\
&=\Omega(n\lg n)
\end{align*}
\]
  <p><strong>Exercise 7.4-5</strong></p>
  <p>We can improve the running time of quicksort in practice by taking advantage of the fast running time of insertion sort when its input is "nearly" sorted. Upon calling quicksort on a subarray with fewer than \(k\) elements, let it simply return without sorting the subarray. After the top-level call to quicksort returns, run insertion sort on the entire array to finish the sorting process. Argue that this sorting algorithm runs in \(O(nk+n\lg(n/k))\) expected time. How should we pick \(k\), both in theory and in practice?</p>
  <p><strong>Exercise 7.4-6 &#9733;</strong></p>
  <p>Consider modifying the \(\text{Partition}\) procedure by randomly picking three elements from array \(A\) and partitioning about their median (the middle value of the three elements). Approximate the probability of getting at worst an \(\alpha\)-to-\((1-\alpha)\) split, as a function of \(\alpha\) in the range \(0 \lt \alpha \lt 1\).</p>
  <p><em>Answer:</em></p>
\[
2(\alpha^3+3\alpha^2(1-\alpha))=6\alpha^2-4\alpha^3
\]
  <h3>Problems</h3>
  <p><strong>Problem 7-1 Hoare partition correctness</strong></p>
  <p>The version of \(\text{Partition}\) given in this chapter is not the original partitioning algorithm. Here is the original partition algorithm, which is due to C. A. R. Hoare:</p>
  <ol style="list-style-type: lower-alpha;">
    <li>Demonstrate the operation of \(\text{HoarePartition}\) on the array \(A=\langle13,19,9,5,12,8,7,4,11,2,6,21\rangle\), showing the values of the array and auxiliary values after each iteration of the \(\textbf{while}\) loop in lines 4&ndash;13.</li>
  </ol>
  <p>The next three questions ask you to give a careful argument that the procedure \(\text{HoarePartition}\) is correct. Assuming that the subarray \(A[p..r]\) contains at least two elements, prove the following:</p>
  <ol style="list-style-type: lower-alpha;" start=2>
    <li>The indices \(i\) and \(j\) are such that we never access an element of \(A\) outside the subarray \(A[p..r]\).</li>
    <li>When \(\text{HoarePartition}\) terminates, it returns a value \(j\) such that \(p\le j\lt r\).</li>
    <li>Every element of \(A[p..j]\) is less than or equal to every element of \(A[j+1..r]\) when \(\text{HoarePartition}\) terminates.</li>
  </ol>
  <p>The \(\text{Partition}\) procedure in Section 7.1 separates the pivot value (originally in \(A[r]\)) from the two partitions it forms. The \(\text{HoarePartition}\) procedure, on the other hand, always places the pivot value (originally in \(A[p]\)) into one of the two partitions \(A[p..j]\) and \(A[j+1..r]\). Since \(p \le j \lt r\), this split is always nontrivial.</p>
  <ol style="list-style-type: lower-alpha;" start=5>
    <li>Rewrite the \(\text{Quicksort}\) procedure to use \(\text{HoarePartition}\).</li>
  </ol>
  <p><em>Answer:</em></p>
  <p>a.</p>
\[
\begin{array}{l|l|ccccccccccccccccc}
i=0&j=13&13 & 19 & 9 & 5 & 12 & 8 & 7 & 4 & 11 & 2 & 6 & 21\\
i=1&j=11&6 & 19 & 9 & 5 & 12 & 8 & 7 & 4 & 11 & 2 & 13 & 21\\
i=2&j=10&6 & 2 & 9 & 5 & 12 & 8 & 7 & 4 & 11 & 19 & 13 & 21\\
i=10&j=9&6 & 2 & 9 & 5 & 12 & 8 & 7 & 4 & 11 & 19 & 13 & 21
\end{array}
\]
  <p><strong>Problem 7-2 Quicksort with equal element values</strong></p>
  <p>The analysis of the expected running time of randomized quicksort in Section 7.4.2 assumes that all element values are distinct. In this problem, we examine what happens when they are not.</p>
  <ol style="list-style-type: lower-alpha;">
    <li>Suppose that all element values are equal. What would be randomized quicksort's running time in this case?</li>
    <li>The \(\text{Partition}\) procedure returns an index \(q\) such that each element of \(A[p..q-1]\) is less than or equal to \(A[q]\) and each element of \(A[q+1..r]\) is greater than \(A[q]\). Modify the \(\text{Partition}\) procedure to produce a procedure \(\text{Partition}'(A,p,r)\), which permutes the elements of \(A[p..r]\) and returns two indices \(q\) and \(t\), where \(p\le q\le t\le r\), such that
      <ul>
        <li>all elements of \(A[q..t]\) are equal,</li>
        <li>each element of \(A[p..q-1]\) is less than \(A[q]\) and</li>
        <li>each element of \(A[t+1..r]\) is greater than \(A[q]\).</li>
      </ul>
      Like \(\text{Partition}\), your \(\text{Partition}'\) procedure should take \(\Theta(r-p)\) time.</li>
    <li>Modify the \(\text{RandomizedQuicksort}\) procedure to call \(\text{Partition}'\), and name the new procedure \(\text{RandomizedQuicksort}'\). Then modify the \(\text{Quicksort}\) procedure to produce a procedure \(\text{Quicksort}'(p,r)\) that calls \(\text{RandomizedQuicksort}'\) and recurses only on partitions of elements not known to be equal to each other.</li>
    <li>Using \(\text{Quicksort}'\), how would you adjust the analysis in Section 7.4.2 to avoid the assumption that all elements are distinct?</li>
  </ol>
  <p><em>Answer:</em></p>
  <p>a.</p>
\[
\Theta(n^2)
\]
  <p><strong>Problem 7-3 Alternative quicksort analysis</strong></p>
  <p>An alternative analysis of the running time of randomized quicksort focuses on the expected running time of each individual recursive call to \(\text{RandomizedQuicksort}\), rather than on the number of comparisons performed.</p>
  <ol style="list-style-type: lower-alpha;">
    <li>Argue that, given an array of size \(n\), the probability that any particular element is chosen as the pivot is \(1/n\). Use this to define indicator random variables \(X_i=\text I\{i\text{th smallest element is chosen as the pivot}\}\). What is \(\text E[X_i]\)?</li>
    <li>Let \(T(n)\) be a random variable denoting the running time of quicksort on an array of size \(n\). Argue that<br>
      \(\text E[T(n)]=\text E\left[\sum\limits_{q=1}^n X_q(T(q-1)+T(n-q)+\Theta(n))\right]\).</li>
    <li>Show that we can rewrite equation (7.5) as<br>
      \(\text E[T(n)]=\dfrac 2 n \sum\limits_{q=2}^{n-1}\text E[T(q)]+\Theta(n)\).</li>
    <li>Show that<br>
      \(\sum\limits_{k=2}^{n-1}k\lg k\le\dfrac12n^2\lg n-\dfrac18n^2\).<br>
      (<em>Hint</em>: Split the summation into two parts, one for \(k=2,3,\dots,\lceil n/2\rceil- 1\) and one for \(k=\lceil n/2\rceil,\dots,n-1\).)</li>
    <li>Using the bound from equation (7.7), show that the recurrence in equation (7.6) has the solution \(\text E[T(n)]=\Theta(n\lg n)\). (<em>Hint</em>: Show, by substitution, that \(\text E[T(n)]\le an \lg n\) for sufficiently large \(n\) and for some positive constant \(a\).)</li>
  </ol>
  <p><em>Answer:</em></p>
  <p>a.</p>
\[
\begin{align*}
\text E[X_i] = \frac 1 n
\end{align*}
\]
  <p>c.</p>
\[
\begin{align*}
\text E[T(n)]
&=\text E\left[\sum_{q=1}^n X_q(T(q-1)+T(n-q)+\Theta(n))\right]\\
&=\text E[X_q]\cdot\text E\left[\sum_{q=1}^n (T(q-1)+T(n-q)+\Theta(n))\right]\\
&=\frac 1n\text E\left[2\sum_{q=1}^n T(q-1)+n\Theta(n)\right]\\
&=\frac 2n\text E\left[\sum_{q=1}^n T(q-1)\right]+\Theta(n)\\
&=\dfrac 2 n \sum_{q=2}^{n-1}\text E[T(q)]+\Theta(n)
\end{align*}
\]
  <p>d.</p>
\[
\begin{align*}
\sum_{k=2}^{n-1}k\lg k
&=\sum_{k=2}^{\lceil n/2\rceil- 1}k\lg k+\sum_{k=\lceil n/2\rceil}^{n-1}k\lg k\\
&\le\sum_{k=2}^{\lceil n/2\rceil- 1}k\lg \frac n2+\sum_{k=\lceil n/2\rceil}^{n-1}k\lg n\\
&=\lg n\sum_{k=2}^{\lceil n/2\rceil- 1}k-\sum_{k=2}^{\lceil n/2\rceil- 1}k+\lg n\sum_{k=\lceil n/2\rceil}^{n-1}k\\
&=\lg n\sum_{k=2}^{n-1}k-\sum_{k=2}^{\lceil n/2\rceil- 1}k\\
&\le\lg n\left(\frac{n^2}2-\frac n2 -1\right)-\left(\frac{n^2}8-\frac n4-1\right)\\
&=\frac12n^2\lg n-\frac18n^2-\left(\frac 12n\lg n+\lg n-\frac n4-1\right)\\
&\le\frac12n^2\lg n-\frac18n^2\\
\end{align*}
\]
  <p>e.</p>
\[
\begin{align*}
\text E[T(n)]
&=\dfrac 2 n \sum_{q=2}^{n-1}\text E[T(q)]+\Theta(n)\\
&\le\dfrac 2 n \sum_{q=2}^{n-1} aq\lg q+\Theta(n)\\
&\le\dfrac {2a} n \left(\frac12n^2\lg n-\frac18n^2\right)+\Theta(n)\\
&=an\lg n-\frac a4n+\Theta(n)\\
&\le an\lg n
\end{align*}
\]
  <p><strong>Problem 7-4 Stack depth for quicksort</strong></p>
  <p>The \(\text{Quicksort}\) algorithm of Section 7.1 contains two recursive calls to itself. After \(\text{Quicksort}\) calls \(\text{Partition}\), it recursively sorts the left subarray and then it recursively sorts the right subarray. The second recursive call in \(\text{Quicksort}\) is not really necessary; we can avoid it by using an iterative control structure. This technique, called <strong><em>tail recursion</em></strong>, is provided automatically by good compilers. Consider the following version of quicksort, which simulates tail recursion:</p>
  <ol style="list-style-type: lower-alpha;">
    <li>Argue that \(\text{TailRecursiveQuicksort}(A,1,A.length)\) correctly sorts the array \(A\).</li>
  </ol>
  <p>Compilers usually execute recursive procedures by using a <strong><em>stack</em></strong> that contains pertinent information, including the parameter values, for each recursive call. The information for the most recent call is at the top of the stack, and the information for the initial call is at the bottom. Upon calling a procedure, its information is <strong><em>pushed</em></strong> onto the stack; when it terminates, its information is <strong><em>popped</em></strong>. Since we assume that array parameters are represented by pointers, the information for each procedure call on the stack requires \(O(1)\) stack space. The <strong><em>stack depth</em></strong> is the maximum amount of stack space used at any time during a computation.</p>
  <ol style="list-style-type: lower-alpha;" start=2>
    <li>Describe a scenario in which \(\text{TailRecursiveQuicksort}\)'s stack depth is \(\Theta(n)\) on an \(n\)-element input array.</li>
    <li>Modify the code for \(\text{TailRecursiveQuicksort}\) so that the worst-case stack depth is \(\Theta(\lg n)\). Maintain the \(O(n\lg n)\) expected running time of the algorithm.</li>
  </ol>
  <p><strong>Problem 7-5 Median-of-\(3\) partition</strong></p>
  <p>One way to improve the \(\text{RandomizedQuicksort}\) procedure is to partition around a pivot that is chosen more carefully than by picking a random element from the subarray. One common approach is the <strong><em>median-of-\(3\)</em></strong> method: choose the pivot as the median (middle element) of a set of \(3\) elements randomly selected from the subarray. (See Exercise 7.4-6.) For this problem, let us assume that the elements in the input array \(A[1..n]\) are distinct and that \(n\ge 3\). We denote the sorted output array by \(A'[1..n]\). Using the median-of-\(3\) method to choose the pivot element \(x\), define \(p_i=\Pr\{x=A'[i]\}\).</p>
  <ol style="list-style-type: lower-alpha;">
    <li>Give an exact formula for \(p_i\) as a function of \(n\) and \(i\) for \(i=2,3,\dots,n-1\). (Note that \(p_1=p_n=0\).)</li>
    <li>By what amount have we increased the likelihood of choosing the pivot as \(x=A'[\lfloor(n+1)/2\rfloor]\), the median of \(A[1..n]\), compared with the ordinary implementation? Assume that \(n\to\infty\), and give the limiting ratio of these probabilities.</li>
    <li>If we define a "good" split to mean choosing the pivot as \(x=A'[i]\), where \(n/3 \le i \le 2n/3\), by what amount have we increased the likelihood of getting a good split compared with the ordinary implementation? (<em>Hint</em>: Approximate the sum by an integral.)</li>
    <li>Argue that in the \(\Omega(n\lg n)\) running time of quicksort, the median-of-\(3\) method affects only the constant factor.</li>
  </ol>
  <p><em>Answer:</em></p>
  <p>a.</p>
\[
\begin{align*}
p_i &= \frac {(i-1)(n-i)}{\begin{pmatrix}n\\3\end{pmatrix}}\\
&=\frac {6(i-1)(n-i)}{n(n-1)(n-2)}
\end{align*}
\]
  <p>b.</p>
\[
\begin{align*}
\lim_{n\to\infty}\frac {6(\lfloor(n+1)/2\rfloor-1)(n-\lfloor(n+1)/2\rfloor)}{n(n-1)(n-2)}/\frac 1n=\frac32
\end{align*}
\]
  <p>c.</p>
\[
\begin{align*}
\lim_{n\to\infty}\sum_{i=n/3}^{2n/3}\frac {6(i-1)(n-i)}{n(n-1)(n-2)}/\frac 13
&\approx \lim_{n\to\infty}3\int_{n/3}^{2n/3}\frac {6(x-1)(n-x)}{n(n-1)(n-2)}dx\\
&=\lim_{n\to\infty}\frac{n(13n-27)}{9(n-2)(n-1)}\\
&=\frac{13}9
\end{align*}
\]
  <p><strong>Problem 7-6 Fuzzy sorting of intervals</strong></p>
  <p>Consider a sorting problem in which we do not know the numbers exactly. Instead, for each number, we know an interval on the real line to which it belongs. That is, we are given \(n\) closed intervals of the form \([a_i,b_i]\), where \(a_i\le b_i\). We wish to <strong><em>fuzzy-sort</em></strong> these intervals, i.e., to produce a permutation \(\langle i_1,i_2,\dots,i_n\rangle\) of the intervals such that for \(j=1,2,\dots,n\), there exist \(c_j\in[a_{i_j},b_{i_j}]\) satisfying \(c_1\le c_2 \le \cdots \le c_n\).</p>
  <ol style="list-style-type: lower-alpha;">
    <li>Design a randomized algorithm for fuzzy-sorting \(n\) intervals. Your algorithm should have the general structure of an algorithm that quicksorts the left endpoints (the \(a_i\) values), but it should take advantage of overlapping intervals to improve the running time. (As the intervals overlap more and more, the problem of fuzzy-sorting the intervals becomes progressively easier. Your algorithm should take advantage of such overlapping, to the extent that it exists.)</li>
    <li>Argue that your algorithm runs in expected time \(\Theta(n\lg n)\) in general, but runs in expected time \(\Theta(n)\) when all of the intervals overlap (i.e., when there exists a value \(x\) such that \(x\in[a_i,b_i]\) for all \(i\)). Your algorithm should not be checking for this case explicitly; rather, its performance should naturally improve as the amount of overlap increases.</li>
  </ol>
  <h2>Chapter 8 Sorting in Linear Time</h2>
  <h3>8.1 Lower bounds for sorting</h3>
  <p><strong>Exercise 8.1-1</strong></p>
  <p>What is the smallest possible depth of a leaf in a decision tree for a comparison sort?</p>
  <p><em>Answer:</em></p>
\[
n-1
\]
  <p><strong>Exercise 8.1-2</strong></p>
  <p>Obtain asymptotically tight bounds on \(\lg(n!)\) without using Stirling's approximation. Instead, evaluate the summation \(\sum_{k=1}^n\lg k\) using techniques from Section A.2.</p>
  <p><em>Answer:</em></p>
\[
\begin{alignat}{4}
&   & \ \lg(n!) &= \sum_{k=1}^n \lg k \\
\sum_{k=\lfloor n/2\rfloor+1}^n \lg k &\le& \ \lg(n!) &\le \sum_{k=1}^n \lg n \\
\frac n 2 \lg \frac n 2 &\le& \ \lg(n!) &\le n\lg n \\
\frac n 2 \lg \frac n {\sqrt n} &\le& \ \lg(n!) &\le n\lg n \\
\frac {n\lg n} 4 &\le& \ \lg(n!) &\le n\lg n \\
\end{alignat}
\]
  <p><strong>Exercise 8.1-3</strong></p>
  <p>Show that there is no comparison sort whose running time is linear for at least half of the \(n!\) inputs of length \(n\). What about a fraction of \(1/n\) of the inputs of length \(n\)? What about a fraction \(1/2^n\)?</p>
  <p><em>Answer:</em></p>
\[
\begin{align*}
\frac{n!}2 &\gt 2^n\\
\frac{n!}n &\gt 2^n\\
\frac{n!}{2^n} &\gt 2^n\\
\end{align*}
\]
  <p><strong>Exercise 8.1-4</strong></p>
  <p>Suppose that you are given a sequence of \(n\) elements to sort. The input sequence consists of \(n/k\) subsequences, each containing \(k\) elements. The elements in a given subsequence are all smaller than the elements in the succeeding subsequence and larger than the elements in the preceding subsequence. Thus, all that is needed to sort the whole sequence of length \(n\) is to sort the \(k\) elements in each of the \(n/k\) subsequences. Show an \(\Omega(n\lg k)\) lower bound on the number of comparisons needed to solve this variant of the sorting problem. (<em>Hint</em>: It is not rigorous to simply combine the lower bounds for the individual subsequences.)</p>
  <p><em>Answer:</em></p>
\[
\begin{align*}
(k!)^{\frac nk} &\le 2^h\\
h &\ge \frac n k \lg(k!)\\
&\ge \frac n k \cdot k\lg k\\
&=n\lg k
\end{align*}
\]
  <h3>8.2 Counting sort</h3>
  <p><strong>Exercise 8.2-1</strong></p>
  <p>Using Figure 8.2 as a model, illustrate the operation of \(\text{CountingSort}\) on the array \(A=\langle 6,0,2,0,1,3,4,6,1,3,2\rangle\).</p>
  <p><em>Answer:</em></p>
\[
\begin{array}{c|ccccccccccccccccccc}
C & 2 & 2 & 2 & 2 & 1 & 0 & 2\\
\hline
C & 2 & 4 & 6 & 8 & 9 & 9 & 11\\
B & \text N & \text N & \text N & \text N & \text N & \text N & \text N & \text N & \text N & \text N & \text N\\
\hline
C & 2 & 4 & 5 & 8 & 9 & 9 & 11\\
B & \text N & \text N & \text N & \text N & \text N & 2 & \text N & \text N & \text N & \text N & \text N\\
\hline
C & 2 & 4 & 5 & 7 & 9 & 9 & 11\\
B & \text N & \text N & \text N & \text N & \text N & 2 & \text N & 3 & \text N & \text N & \text N\\
\hline
C & 2 & 3 & 5 & 7 & 9 & 9 & 11\\
B & \text N & \text N & \text N & 1 & \text N & 2 & \text N & 3 & \text N & \text N & \text N\\
\hline
C & 2 & 3 & 5 & 7 & 9 & 9 & 10\\
B & \text N & \text N & \text N & 1 & \text N & 2 & \text N & 3 & \text N & \text N & 6\\
\hline
C & 2 & 3 & 5 & 7 & 8 & 9 & 10\\
B & \text N & \text N & \text N & 1 & \text N & 2 & \text N & 3 & 4 & \text N & 6\\
\hline
C & 2 & 3 & 5 & 6 & 8 & 9 & 10\\
B & \text N & \text N & \text N & 1 & \text N & 2 & 3 & 3 & 4 & \text N & 6\\
\hline
C & 2 & 2 & 5 & 6 & 8 & 9 & 10\\
B & \text N & \text N & 1 & 1 & \text N & 2 & 3 & 3 & 4 & \text N & 6\\
\hline
C & 1 & 2 & 5 & 6 & 8 & 9 & 10\\
B & \text N & 0 & 1 & 1 & \text N & 2 & 3 & 3 & 4 & \text N & 6\\
\hline
C & 1 & 2 & 4 & 6 & 8 & 9 & 10\\
B & \text N & 0 & 1 & 1 & 2 & 2 & 3 & 3 & 4 & \text N & 6\\
\hline
C & 0 & 2 & 4 & 6 & 8 & 9 & 10\\
B & 0 & 0 & 1 & 1 & 2 & 2 & 3 & 3 & 4 & \text N & 6\\
\hline
C & 0 & 2 & 4 & 6 & 8 & 9 & 9\\
B & 0 & 0 & 1 & 1 & 2 & 2 & 3 & 3 & 4 & 6 & 6
\end{array}
\]
  <p><strong>Exercise 8.2-2</strong></p>
  <p>Prove that \(\text{CountingSort}\) is stable.</p>
  <p><strong>Exercise 8.2-3</strong></p>
  <p>Suppose that we were to rewrite the \(\textbf{for}\) loop header in line 10 of the \(\text{CountingSort}\) as<br>
    \(10\quad\textbf{for }j=1\textbf{ to }A.length\)<br>
    Show that the algorithm still works properly. Is the modified algorithm stable?</p>
  <p><strong>Exercise 8.2-4</strong></p>
  <p>Describe an algorithm that, given \(n\) integers in the range \(0\) to \(k\), preprocesses its input and then answers any query about how many of the \(n\) integers fall into a range \([a..b]\) in \(O(1)\) time. Your algorithm should use \(\Theta(n+k)\) preprocessing time.</p>
  <h3>8.3 Radix sort</h3>
  <p><strong>Exercise 8.3-1</strong></p>
  <p>Using Figure 8.3 as a model, illustrate the operation of \(\text{RadixSort}\) on the following list of English words: \(\text{COW, DOG, SEA, RUG, ROW, MOB, BOX, TAB, BAR, EAR, TAR, DIG, BIG, TEA, NOW, FOX}\).</p>
  <p><em>Answer:</em></p>
\[
\begin{array}{}
\text{COW} & \text{DOG} & \text{SEA} & \text{RUG} & \text{ROW} & \text{MOB} & \text{BOX} & \text{TAB} & \text{BAR} & \text{EAR} & \text{TAR} & \text{DIG} & \text{BIG} & \text{TEA} & \text{NOW} & \text{FOX}\\
\text{SEA} & \text{TEA} & \text{MOB} & \text{TAB} & \text{DOG} & \text{RUG} & \text{DIG} & \text{BIG} & \text{BAR} & \text{EAR} & \text{TAR} & \text{COW} & \text{ROW} & \text{NOW} & \text{BOX} & \text{FOX}\\
\text{TAB} & \text{BAR} & \text{EAR} & \text{TAR} & \text{SEA} & \text{TEA} & \text{DIG} & \text{BIG} & \text{MOB} & \text{DOG} & \text{COW} & \text{ROW} & \text{NOW} & \text{BOX} & \text{FOX} & \text{RUG}\\
\text{BAR} & \text{BIG} & \text{BOX} & \text{COW} & \text{DIG} & \text{DOG} & \text{EAR} & \text{FOX} & \text{MOB} & \text{NOW} & \text{ROW} & \text{RUG} & \text{SEA} & \text{TAB} & \text{TAR} & \text{TEA}
\end{array}
\]
  <p><strong>Exercise 8.3-2</strong></p>
  <p>Which of the following sorting algorithms are stable: insertion sort, merge sort, heapsort, and quicksort? Give a simple scheme that makes any sorting algorithm stable. How much additional time and space does your scheme entail?</p>
  <p><em>Answer:</em></p>
  <p>insertion sort and merge sort are stable; heapsort and quicksort are not.</p>
  <p><strong>Exercise 8.3-3</strong></p>
  <p>Use induction to prove that radix sort works. Where does your proof need the assumption that the intermediate sort is stable?</p>
  <p><strong>Exercise 8.3-4</strong></p>
  <p>Show how to sort \(n\) integers in the range \(0\) to \(n^3-1\) in \(O(n)\) time.</p>
  <p><em>Answer:</em></p>
  <p>insertion sort and merge sort are stable; heapsort and quicksort are not.</p>
  <p><strong>Exercise 8.3-5 &#9733;</strong></p>
  <p>In the first card-sorting algorithm in this section, exactly how many sorting passes are needed to sort \(d\)-digit decimal numbers in the worst case? How many piles of cards would an operator need to keep track of in the worst case?</p>
  <p><em>Answer:</em></p>
\[
d\\10
\]
  <h3>8.4 Bucket sort</h3>
  <p><strong>Exercise 8.4-1</strong></p>
  <p>Using Figure 8.4 as a model, illustrate the operation of \(\text{BucketSort}\) on the array \(A=\langle .79,.13,.16,.64,.39,.20,.89,.53,.71,.42\rangle\).</p>
  <p><em>Answer:</em></p>
\[
\begin{array}{}
0\\
1& 0.13 & 0.16\\
2& 0.2\\
3& 0.39\\
4& 0.42\\
5& 0.53\\
6& 0.64\\
7& 0.71&0.79\\
8& 0.89\\
9
\end{array}
\]
  <p><strong>Exercise 8.4-2</strong></p>
  <p>Explain why the worst-case running time for bucket sort is \(\Theta(n^2)\). What simple change to the algorithm preserves its linear average-case running time and makes its worst-case running time \(O(n \lg n)\)?</p>
  <p><strong>Exercise 8.4-3</strong></p>
  <p>Let \(X\) be a random variable that is equal to the number of heads in two flips of a fair coin. What is \(\text E[X^2]\)? What is \(\text E^2[X]\)?</p>
  <p><em>Answer:</em></p>
\[
\begin{align*}
0^2 \cdot \frac 1 4 + 1^2 \cdot \frac 1 2 + 2^2 \cdot \frac 1 4 &= \frac 3 2\\
\left(0 \cdot \frac 1 4 + 1 \cdot \frac 1 2 + 2 \cdot \frac 1 4\right)^2 &= 1
\end{align*}
\]
  <p><strong>Exercise 8.4-4 &#9733;</strong></p>
  <p>We are given \(n\) points in the unit circle, \(p_i=(x_i,y_i)\), such that \(0\lt x_i^2+y_i^2 \le 1\) for \(i=1,2,\dots,n\). Suppose that the points are uniformly distributed; that is, the probability of finding a point in any region of the circle is proportional to the area of that region. Design an algorithm with an average-case running time of \(\Theta(n)\) to sort the \(n\) points by their distances \(d_i=\sqrt{x_i^2+y_i^2}\) from the origin. (<em>Hint</em>: Design the bucket sizes in \(\text{BucketSort}\) to reflect the uniform distribution of the points in the unit circle.)</p>
  <p><strong>Exercise 8.4-5 &#9733;</strong></p>
  <p>A <strong><em>probability distribution function</em></strong> \(P(x)\) for a random variable \(X\) is defined by \(P(x)=\Pr\{X\le x\}\). Suppose that we draw a list of \(n\) random variables \(X_1,X_2,\dots,X_n\) from a continuous probability distribution function \(P\) that is computable in \(O(1)\) time. Give an algorithm that sorts these numbers in linear average-case time.</p>
  <h3>Problems</h3>
  <p><strong>Problem 8-1 Probabilistic lower bounds on comparison sorting</strong></p>
  <p>In this problem, we prove a probabilistic \(\Omega(n\lg n)\) lower bound on the running time of any deterministic or randomized comparison sort on \(n\) distinct input elements. We begin by examining a deterministic comparison sort \(A\) with decision tree \(T_A\). We assume that every permutation of \(A\)'s inputs is equally likely.</p>
  <ol style="list-style-type: lower-alpha;">
    <li>Suppose that each leaf of \(T_A\) is labeled with the probability that it is reached given a random input. Prove that exactly \(n!\) leaves are labeled \(1/n!\) and that the rest are labeled \(0\).</li>
    <li>Let \(D(T)\) denote the external path length of a decision tree \(T\); that is, \(D(T)\) is the sum of the depths of all the leaves of \(T\). Let \(T\) be a decision tree with \(k\gt 1\) leaves, and let \(LT\) and \(RT\) be the left and right subtrees of \(T\). Show that \(D(T)=D(LT)+D(RT)+k\).</li>
    <li>Let \(d(k)\) be the minimum value of \(D(T)\) over all decision trees \(T\) with \(k\gt 1\) leaves. Show that \(d(k)=\min_{1\le i\le k-1} \{d(i)+d(k-i)+k\}\). (<em>Hint</em>: Consider a decision tree \(T\) with \(k\) leaves that achieves the minimum. Let \(i_0\) be the number of leaves in \(LT\) and \(k-i_0\) the number of leaves in \(RT\).)</li>
    <li>Prove that for a given value of \(k\gt 1\) and \(i\) in the range \(1\le i\le k-1\), the function \(i\lg i+(k-i)\lg(k-i)\) is minimized at \(i=k/2\). Conclude that \(d(k)=k\lg k\).</li>
    <li>Prove that \(D(T_A)=\Omega(n!\lg(n!))\), and conclude that the average-case time to sort \(n\) elements is \(\Omega(n\lg n)\).</li>
  </ol>
  <p>Now, consider a <em>randomized</em> comparison sort \(B\). We can extend the decision-tree model to handle randomization by incorporating two kinds of nodes: ordinary comparison nodes and "randomization" nodes. A randomization node models a random choice of the form \(\text{Random}(1,r)\) made by algorithm \(B\); the node has \(r\) children, each of which is equally likely to be chosen during an execution of the algorithm.</p>
  <ol style="list-style-type: lower-alpha;" start=6>
    <li>Show that for any randomized comparison sort \(B\), there exists a deterministic comparison sort \(A\) whose expected number of comparisons is no more than those made by \(B\).</li>
  </ol>
  <p><strong>Problem 8-2 Sorting in place in linear time</strong></p>
  <p>Suppose that we have an array of \(n\) data records to sort and that the key of each record has the value \(0\) or \(1\). An algorithm for sorting such a set of records might possess some subset of the following three desirable characteristics:</p>
  <ol>
    <li>The algorithm runs in \(O(n)\) time.</li>
    <li>The algorithm is stable.</li>
    <li>The algorithm sorts in place, using no more than a constant amount of storage space in addition to the original array.</li>
  </ol>
  <ol style="list-style-type: lower-alpha;">
    <li>Give an algorithm that satisfies criteria 1 and 2 above.</li>
    <li>Give an algorithm that satisfies criteria 1 and 3 above.</li>
    <li>Give an algorithm that satisfies criteria 2 and 3 above.</li>
    <li>Can you use any of your sorting algorithms from parts (a)–(c) as the sorting method used in line 2 of \(\text{RadixSort}\), so that \(\text{RadixSort}\) sorts \(n\) records with \(b\)-bit keys in \(O(bn)\) time? Explain how or why not.</li>
    <li>Suppose that the \(n\) records have keys in the range from \(1\) to \(k\). Show how to modify counting sort so that it sorts the records in place in \(O(n+k)\) time. You may use \(O(k)\) storage outside the input array. Is your algorithm stable? (<em>Hint</em>: How would you do it for \(k=3\)?)</li>
  </ol>
  <p><em>Answer:</em></p>
  <p>a. counting sort</p>
  <p>b. hoare partition</p>
  <p>c. insertion sort</p>
  <p>d. (a)</p>
  <p>e. counting sort</p>
  <p><strong>Problem 8-3 Sorting variable-length items</strong></p>
  <ol>
    <li>You are given an array of integers, where different integers may have different numbers of digits, but the total number of digits over all the integers in the array is \(n\). Show how to sort the array in \(O(n)\) time.</li>
    <li>You are given an array of strings, where different strings may have different numbers of characters, but the total number of characters over all the strings is \(n\). Show how to sort the strings in \(O(n)\) time.<br>
      (Note that the desired order here is the standard alphabetical order; for example, \(\mathtt{a\lt ab\lt b}\).)</li>
  </ol>
  <p><em>Answer:</em></p>
  <p>a. bucket sort + counting sort</p>
  <p>b. reverse every string + radix sort + reverse back</p>
  <p><strong>Problem 8-4 Water jugs</strong></p>
  <p>Suppose that you are given \(n\) red and \(n\) blue water jugs, all of different shapes and sizes. All red jugs hold different amounts of water, as do the blue ones. Moreover, for every red jug, there is a blue jug that holds the same amount of water, and vice versa.</p>
  <p>Your task is to find a grouping of the jugs into pairs of red and blue jugs that hold the same amount of water. To do so, you may perform the following operation: pick a pair of jugs in which one is red and one is blue, fill the red jug with water, and then pour the water into the blue jug. This operation will tell you whether the red or the blue jug can hold more water, or that they have the same volume. Assume that such a comparison takes one time unit. Your goal is to find an algorithm that makes a minimum number of comparisons to determine the grouping. Remember that you may not directly compare two red jugs or two blue jugs.</p>
  <ol  style="list-style-type: lower-alpha;">
    <li>Describe a deterministic algorithm that uses \(\Theta(n^2)\) comparisons to group the jugs into pairs.</li>
    <li>Prove a lower bound of \(\Omega(n\lg n)\) for the number of comparisons that an algorithm solving this problem must make.</li>
    <li>Give a randomized algorithm whose expected number of comparisons is \(O(n\lg n)\), and prove that this bound is correct. What is the worst-case number of comparisons for your algorithm?</li>
  </ol>
  <p><em>Answer:</em></p>
  <p>c. quicksort</p>
  <p><strong>Problem 8-5 Average sorting</strong></p>
  <p>Suppose that, instead of sorting an array, we just require that the elements increase on average. More precisely, we call an \(n\)-element array \(A\) <strong><em>\(k\)-sorted</em></strong> if, for all \(i=1,2,\dots,n-k\), the following holds:</p>
  <p>\(\dfrac{\sum_{j=i}^{i+k-1}A[j]}k\le \dfrac{\sum_{j=i+1}^{i+k}A[j]}k\).</p>
  <ol  style="list-style-type: lower-alpha;">
    <li>What does it mean for an array to be \(1\)-sorted?</li>
    <li>Give a permutation of the numbers \(1,2,\dots,10\) that is \(2\)-sorted, but not sorted.</li>
    <li>Prove that an \(n\)-element array is \(k\)-sorted if and only if \(A[i]\le A[i+k]\) for all \(i=1,2,\dots,n-k\).</li>
    <li>Give an algorithm that \(k\)-sorts an \(n\)-element array in \(O(n\lg(n/k))\) time.</li>
  </ol>
  <p>We can also show a lower bound on the time to produce a \(k\)-sorted array, when \(k\) is a constant.</p>
  <ol  style="list-style-type: lower-alpha;" start=5>
    <li>Show that we can sort a \(k\)-sorted array of length \(n\) in \(O(n\lg k)\) time. (<em>Hint</em>: Use the solution to Exercise 6.5-9.)</li>
    <li>Show that when \(k\) is a constant, \(k\)-sorting an \(n\)-element array requires \(O(n\lg n)\) time. (<em>Hint</em>: Use the solution to the previous part along with the lower bound on comparison sorts.)</li>
  </ol>
  <p><em>Answer:</em></p>
  <p>c.</p>
\[
\begin{align*}
\dfrac{\sum_{j=i}^{i+k-1}A[j]}k&\le \dfrac{\sum_{j=i+1}^{i+k}A[j]}k\\
\sum_{j=i}^{i+k-1}A[j]&\le \sum_{j=i+1}^{i+k}A[j]\\
A[i]+\sum_{j=i+1}^{i+k-1}A[j]&\le \sum_{j=i+1}^{i+k-1}A[j]+A[i+k]\\
A[i]&\le A[i+k]
\end{align*}
\]
  <p><strong>Problem 8-6 Lower bound on merging sorted lists</strong></p>
  <p>The problem of merging two sorted lists arises frequently. We have seen a procedure for it as the subroutine \(\text{Merge}\) in Section 2.3.1. In this problem, we will prove a lower bound of \(2n-1\) on the worst-case number of comparisons required to merge two sorted lists, each containing \(n\) items.</p>
  <p>First we will show a lower bound of \(2n-o(n)\) comparisons by using a decision tree.</p>
  <ol  style="list-style-type: lower-alpha;">
    <li>Given \(2n\) numbers, compute the number of possible ways to divide them into two sorted lists, each with \(n\) numbers.</li>
    <li>Using a decision tree and your answer to part (a), show that any algorithm that correctly merges two sorted lists must perform at least \(2n-o(n)\) comparisons.</li>
  </ol>
  <p>Now we will show a slightly tighter \(2n-1\) bound.</p>
  <ol  style="list-style-type: lower-alpha;" start=3>
    <li>Show that if two elements are consecutive in the sorted order and from different
lists, then they must be compared.</li>
    <li>Use your answer to the previous part to show a lower bound of \(2n-1\) comparisons
for merging two sorted lists.
</li>
  </ol>
  <p><em>Answer:</em></p>
  <p>a. \(\begin{pmatrix}2n\\n\end{pmatrix}\)</p>
  <p>b.</p>
\[
\begin{align*}
\lg\left(\begin{pmatrix}2n\\n\end{pmatrix}\right)
&= \lg\left(\frac {2^{2n}}{\sqrt{\pi n}}\left(1+O\left(\frac 1 n\right)\right)\right)\\
&=\lg(2^{2n})-\left(\lg(\sqrt{\pi n})-\lg\left(1+O\left(\frac 1 n\right)\right)\right)\\
&=2n-o(n)\\
\end{align*}
\]
  <p><strong>Problem 8-7 The 0-1 sorting lemma and columnsort</strong></p>
  <p>A compare-exchange operation on two array elements \(A[i]\) and \(A[j]\), where \(i\lt j\), has the form</p>
  <pre>def compare_exchange(a, i, j):
    if a[i] > a[j]:
        a[i], a[j] = a[j], a[i]</pre>
  <p>After the compare-exchange operation, we know that \(A[i]\le A[j]\).</p>
  <p>An <strong><em>oblivious compare-exchange algorithm</em></strong> operates solely by a sequence of prespecified compare-exchange operations. The indices of the positions compared in the sequence must be determined in advance, and although they can depend on the number of elements being sorted, they cannot depend on the values being sorted, nor can they depend on the result of any prior compare-exchange operation. For example, here is insertion sort expressed as an oblivious compare-exchange algorithm:</p>
  <pre>def insertion_sort(a):
    for j in range(1, len(a)):
        for i in range(j - 1, -1, -1):
            compare_exchange(a, i, i + 1)</pre>
  <p>The <strong><em>0-1 sorting lemma</em></strong> provides a powerful way to prove that an oblivious compare-exchange algorithm produces a sorted result. It states that if an oblivious compare-exchange algorithm correctly sorts all input sequences consisting of only \(0\)s and \(1\)s, then it correctly sorts all inputs containing arbitrary values.</p>
  <p>You will prove the 0-1 sorting lemma by proving its contrapositive: if an oblivious compare-exchange algorithm fails to sort an input containing arbitrary values, then it fails to sort some 0-1 input. Assume that an oblivious compare-exchange algorithm \(X\) fails to correctly sort the array \(A[1..n]\). Let \(A[p]\) be the smallest value in \(A\) that algorithm \(X\) puts into the wrong location, and let \(A[q]\) be the value that algorithm \(X\) moves to the location into which \(A[p]\) should have gone. Define an array \(B[1..n]\) of \(0\)s and \(1\)s as follows:</p>
  <p>\(
B[i]=\left\{\begin{array}{}0&\text{if }A[i]\le A[p],\\
1&\text{if }A[i]\gt A[p].\end{array}\right.
\)</p>
  <ol style="list-style-type: lower-alpha;">
    <li>Argue that \(A[q]\gt A[p]\), so that \(B[p]=0\) and \(B[q]=1\).</li>
    <li>To complete the proof of the 0-1 sorting lemma, prove that algorithm \(X\) fails to sort array \(B\) correctly.</li>
  </ol>
  <p>Now you will use the 0-1 sorting lemma to prove that a particular sorting algorithm works correctly. The algorithm, <strong><em>columnsort</em></strong>, works on a rectangular array of \(n\) elements. The array has \(r\) rows and \(s\) columns (so that \(n=rs\)), subject to three restrictions:</p>
  <ul>
    <li>\(r\) must be even,</li>
    <li>\(s\) must be a divisor of \(r\), and</li>
    <li>\(r\ge 2s^2\).</li>
  </ul>
  <p>When columnsort completes, the array is sorted in <strong><em>column-major order</em></strong>: reading down the columns, from left to right, the elements monotonically increase.</p>
  <p>Columnsort operates in eight steps, regardless of the value of \(n\). The odd steps are all the same: sort each column individually. Each even step is a fixed permutation. Here are the steps:</p>
  <ol>
    <li>Sort each column.</li>
    <li>Transpose the array, but reshape it back to \(r\) rows and \(s\) columns. In other words, turn the leftmost column into the top \(r/s\) rows, in order; turn the next column into the next \(r/s\) rows, in order; and so on.</li>
    <li>Sort each column.</li>
    <li>Perform the inverse of the permutation performed in step 2</li>
    <li>Sort each column.</li>
    <li>Shift the top half of each column into the bottom half of the same column, and
shift the bottom half of each column into the top half of the next column to the
right. Leave the top half of the leftmost column empty. Shift the bottom half
of the last column into the top half of a new rightmost column, and leave the
bottom half of this new column empty.</li>
    <li>Sort each column.</li>
    <li>Perform the inverse of the permutation performed in step 6.</li>
  </ol>
  <p>Figure 8.5 shows an example of the steps of columnsort with \(r=6\) and \(s=3\). (Even though this example violates the requirement that \(r\ge 2s^2\), it happens to work.)</p>
  <table style="width: 100%;">
    <tbody>
      <tr>
        <td style="text-align: center;">\(\begin{array}{}10&14&5\\8&7&17\\12&1&6\\16&9&11\\4&15&2\\18&3&13\end{array}\)<br>(a)</td>
        <td style="text-align: center;">\(\begin{array}{}4&1&2\\8&3&5\\10&7&6\\12&9&11\\16&14&13\\18&15&17\end{array}\)<br>(b)</td>
        <td style="text-align: center;">\(\begin{array}{}4 & 8 & 10\\12 & 16 & 18\\1 & 3 & 7\\9 & 14 & 15\\2 & 5 & 6\\11 & 13 & 17\end{array}\)<br>(c)</td>
        <td style="text-align: center;">\(\begin{array}{}1 & 3 & 6\\2 & 5 & 7\\4 & 8 & 10\\9 & 13 & 15\\11 & 14 & 17\\12 & 16 & 18\end{array}\)<br>(d)</td>
        <td style="text-align: center;">\(\begin{array}{}1 & 4 & 11\\3 & 8 & 14\\6 & 10 & 17\\2 & 9 & 12\\5 & 13 & 16\\7 & 15 & 18\end{array}\)<br>(e)</td>
      </tr>
      <tr>
        <td style="text-align: center;">\(\begin{array}{}1 & 4 & 11\\2 & 8 & 12\\3 & 9 & 14\\5 & 10 & 16\\6 & 13 & 17\\7 & 15 & 18\end{array}\)<br>(f)</td>
        <td style="text-align: center;">\(\begin{array}{}&5 & 10 & 16\\&6 & 13 & 17\\&7 & 15 & 18\\1 & 4 & 11\\2 & 8 & 12\\3 & 9 & 14\end{array}\)<br>(g)</td>
        <td style="text-align: center;">\(\begin{array}{}&4 & 10 & 16\\&5 & 11 & 17\\&6 & 12 & 18\\1 & 7 & 13\\2 & 8 & 14\\3 & 9 & 15\end{array}\)<br>(h)</td>
        <td style="text-align: center;">\(\begin{array}{}1 & 7 & 13\\2 & 8 & 14\\3 & 9 & 15\\4 & 10 & 16\\5 & 11 & 17\\6 & 12 & 18\end{array}\)<br>(i)</td>
      </tr>
    </tbody>
    <caption style="text-align: left; caption-side:bottom; font-size: small"><strong>Figure 8.5</strong> The steps of columnsort. <strong>(a)</strong> The input array with \(6\) rows and \(3\) columns. <strong>(b)</strong> After sorting each column in step 1. <strong>(c)</strong> After transposing and reshaping in step 2. <strong>(d)</strong> After sorting each column in step 3. <strong>(e)</strong> After performing step 4, which inverts the permutation from step 2. <strong>(f)</strong> After sorting each column in step 5. <strong>(g)</strong> After shifting by half a column in step 6. <strong>(h)</strong> After sorting each column in step 7. <strong>(i)</strong> After performing step 8, which inverts the permutation from step 6. The array is now sorted in column-major order.</caption>
  </table>
  <ol style="list-style-type: lower-alpha;" start=3>
    <li>Argue that we can treat columnsort as an oblivious compare-exchange algorithm, even if we do not know what sorting method the odd steps use.</li>
  </ol>
  <p>Although it might seem hard to believe that columnsort actually sorts, you will use the 0-1 sorting lemma to prove that it does. The 0-1 sorting lemma applies because we can treat columnsort as an oblivious compare-exchange algorithm. A couple of definitions will help you apply the 0-1 sorting lemma. We say that an area of an array is <strong><em>clean</em></strong> if we know that it contains either all \(0\)s or all \(1\)s. Otherwise, the area might contain mixed \(0\)s and \(1\)s, and it is <strong><em>dirty</em></strong>. From here on, assume that the input array contains only \(0\)s and \(1\)s, and that we can treat it as an array with \(r\) rows and \(s\) columns.</p>
  <ol style="list-style-type: lower-alpha;" start=4>
    <li>Prove that after steps 1–3, the array consists of some clean rows of \(0\)s at the top, some clean rows of \(1\)s at the bottom, and at most \(s\) dirty rows between them.</li>
    <li>Prove that after step 4, the array, read in column-major order, starts with a clean
area of \(0\)s, ends with a clean area of \(1\)s, and has a dirty area of at most \(s^2\) elements in the middle.</li>
    <li>Prove that steps 5–8 produce a fully sorted 0-1 output. Conclude that columnsort correctly sorts all inputs containing arbitrary values.</li>
    <li>Now suppose that \(s\) does not divide \(r\). Prove that after steps 1–3, the array consists of some clean rows of \(0\)s at the top, some clean rows of \(1\)s at the bottom, and at most \(2s-1\) dirty rows between them. How large must \(r\) be, compared with \(s\), for columnsort to correctly sort when \(s\) does not divide \(r\)?</li>
    <li>Suggest a simple change to step 1 that allows us to maintain the requirement that \(r\ge2s^2\) even when \(s\) does not divide \(r\), and prove that with your change, columnsort correctly sorts.</li>
  </ol>
  <p><em>Answer:</em></p>
  <p>g. \(2s^2-2s\)</p>









  <h2>Chapter 9 Medians and Order Statistics</h2>

  <h3>9.1 Minimum and maximum</h3>

  <pre>def minimum(a):
    mini = a[0]
    for i in range(1, len(a)):
        if mini &gt; a[i]:
            mini = a[i]
    return mini</pre>

  <pre>def minimum_and_maximum(a):
    if len(a) % 2:
        mini = maxi = a[0]
        start = 1
    elif a[0] &gt; a[1]:
        maxi = a[0]
        mini = a[1]
        start = 2
    else:
        maxi = a[1]
        mini = a[0]
        start = 2
    for i in range(start, len(a), 2):
        j = i + 1
        if a[i] &gt; a[j]:
            big = a[i]
            small = a[j]
        else:
            big = a[j]
            small = a[i]
        if big &gt; maxi:
            maxi = big
        if small &lt; mini:
            mini = small
    return mini, maxi</pre>

  <p><strong>Exercise 9.1-1</strong></p>
  <p>Show that the second smallest of \(n\) elements can be found with \(n+\lceil \lg n\rceil-2\) comparisons in the worst case. (<em>Hint</em>: Also find the smallest element.)</p>
  <p><em>Answer:</em></p>
  <pre>def smallest_two(a):
    record = [[] for x in a]
    curr = list(range(len(a)))
    while len(curr) > 1:
        nxt = []
        for k in range(0, len(curr)-1, 2):
            i = curr[k]
            j = curr[k+1]
            if a[i] &lt; a[j]:
                record[i].append(j)
                nxt.append(i)
            else:
                record[j].append(i)
                nxt.append(j)
        if len(curr) % 2:
            nxt.append(curr[-1])
        curr = nxt
    champion = curr[0]
    loser = record[champion]
    second = loser[0]
    for k in range(1, len(loser)):
        ind = loser[k]
        if a[ind] &lt; a[second]:
            second = ind
    return champion, second</pre>
\[
\begin{align*}
T(n) &= (n - 1) + (\lceil \lg n\rceil - 1)\\
&= n+\lceil \lg n\rceil-2\
\end{align*}
\]

  <p><strong>Exercise 9.1-2 &#9733;</strong></p>
  <p>Prove the lower bound of \(\lceil 3n/2\rceil-2\) comparisons in the worst case to find both the maximum and minimum of \(n\) numbers. (<em>Hint</em>: Consider how many numbers are potentially either the maximum or minimum, and investigate how a comparison affects these counts.)</p>
  <p><em>Answer:</em></p>
\[
\begin{align*}
T(n) &= \left\lfloor \frac n 2 \right\rfloor + \left(n - \left\lfloor \frac n 2 \right\rfloor-1\right)+ \left(n - \left\lfloor \frac n 2 \right\rfloor-1\right)\\
&= \left\lceil \frac {3n} 2 \right\rceil - 2
\end{align*}
\]

  <h3>9.2 Selection in expected linear time</h3>

  <pre>def randomized_select(a, p, r, i):
    if r - p == 1:
        return a[p]
    q = randomized_partition(a, p, r)
    k = q - p
    if i == k:
        return a[q]
    if i &lt; k:
        return randomized_select(a, p, q, i)
    return randomized_select(a, q + 1, r, i - k - 1)</pre>

  <p><strong>Exercise 9.2-1</strong></p>
  <p>Show that \(\text{RandomizedSelect}\) never makes a recursive call to a \(0\)-length array.</p>
  <p><em>Answer:</em></p>
\[
\begin{align*}
k\gt i&\ge1\\
k&\ge2\\
q-p+1 &\ge 2\\
q-1&\ge p\\
\\
k\lt i &\le r - p + 1\\
k &\le r - p \\
q-p+1&\le r - p\\
q +1&\le r
\end{align*}
\]

  <p><strong>Exercise 9.2-2</strong></p>
  <p>Argue that the indicator random variable \(X_k\) and the value \(T(\max(k-1,n-k))\) are independent.</p>

  <p><strong>Exercise 9.2-3</strong></p>
  <p>Write an iterative version of \(\text{RandomizedSelect}\).</p>
  <p><em>Answer:</em></p>
  <pre>def randomized_select(a, p, r, i):
    while r - p &gt; 1:
        q = randomized_partition(a, p, r)
        k = q - p
        if i == k:
            return a[q]
        if i &lt; k:
            r = q
        else:
            p = q + 1
            i -= k + 1
    return a[p]</pre>

  <p><strong>Exercise 9.2-4</strong></p>
  <p>Suppose we use \(\text{RandomizedSelect}\) to select the minimum element of the array \(A=\langle 3,2,9,0,7,5,4,8,6,1\rangle\). Describe a sequence of partitions that results in a worst-case performance of \(\text{RandomizedSelect}\).</p>
  <p><em>Answer:</em></p>
\[
q = 10, 9, 8, 7, 6, 5, 4, 3, 2
\]

  <h3>9.3 Selection in worst-case linear time</h3>

  <p><strong>Exercise 9.3-1</strong></p>
  <p>In the algorithm \(\text{Select}\), the input elements are divided into groups of \(5\). Will the algorithm work in linear time if they are divided into groups of \(7\)? Argue that \(\text{Select}\) does not run in linear time if groups of \(3\) are used.</p>
  <p><em>Answer:</em></p>
\[
\begin{align*}
n-4\left(\left\lceil\frac 1 2 \left\lceil\frac n 7\right\rceil\right\rceil-2\right)
&\le \frac {5n}7 + 8\\
T(n) &\le T\left(\left\lceil\frac n 7\right\rceil\right) + T\left( \frac {5n}7 + 8\right)+O(n)\\
&\le \frac {cn} 7 + c+ \frac {5cn}7 + 8c +an\\
&\le cn -\left(\frac {cn} 7 - 9c -an\right)\\
&\le cn \qquad(n \ge 126, c \ge 14a)\\
\\
n-2\left(\left\lceil\frac 1 2 \left\lceil\frac n 3\right\rceil\right\rceil-2\right)
&\le \frac {2n}3 + 4\\
T(n) &= T\left(\left\lceil\frac n 3\right\rceil\right) + T\left( \frac {2n}3 + 4\right)+\Omega(n)\\
&\ge T\left(\frac n 3\right) + T\left( \frac {2n}3\right) +\Omega(n)\\
&\ge \frac {cn} 3\lg \frac n 3 + \frac {2cn}3\lg\frac{2n}3 + an\\
&\ge cn \lg n + \left(a + c\left(\frac 2 3 - \lg 3\right)\right)n\\
&\ge cn \lg n\\
\end{align*}
\]

  <p><strong>Exercise 9.3-2</strong></p>
  <p>Analyze \(\text{Select}\) to show that if \(n\ge 140\), then at least \(\lceil n/4\rceil\) elements are greater than the median-of-medians \(x\) and at least \(\lceil n/4\rceil\) elements are less than \(x\).</p>
  <p><em>Answer:</em></p>
\[
\begin{align*}
\frac {3n}{10}-6 &= \frac n 4 + \frac n{20} -6\\
&\ge \frac n 4 + 1\\
&\ge \lceil n/4\rceil\\
\end{align*}
\]

  <p><strong>Exercise 9.3-3</strong></p>
  <p>Show how quicksort can be made to run in \(O(n\lg n)\) time in the worst case, assuming that all elements are distinct.</p>
  <p><em>Answer:</em> Find median in O(n) time, use it as pivot.</p>

  <p><strong>Exercise 9.3-4 &#9733;</strong></p>
  <p>Suppose that an algorithm uses only comparisons to find the \(i\)th smallest element in a set of \(n\) elements. Show that it can also find the \(i-1\) smaller elements and the \(n-i\) larger elements without performing any additional comparisons.</p>

  <p><strong>Exercise 9.3-5</strong></p>
  <p>Suppose that you have a "black-box" worst-case linear-time median subroutine. Give a simple, linear-time algorithm that solves the selection problem for an arbitrary order statistic.</p>

  <p><strong>Exercise 9.3-6</strong></p>
  <p>The \(k\)th <strong><em>quantiles</em></strong> of an \(n\)-element set are the \(k-1\) order statistics that divide the sorted set into \(k\) equal-sized sets (to within \(1\)). Give an \(O(n\lg k)\)-time algorithm to list the \(k\)th quantiles of a set.</p>

  <p><strong>Exercise 9.3-7</strong></p>
  <p>Describe an \(O(n)\)-time algorithm that, given a set \(S\) of \(n\) distinct numbers and a positive integer \(k \le n\), determines the \(k\) numbers in \(S\) that are closest to the median of \(S\).</p>

  <p><strong>Exercise 9.3-8</strong></p>
  <p>Let \(X[1..n]\) and \(Y[1..n]\) be two arrays, each containing \(n\) numbers already in sorted order. Give an \(O(\lg n)\)-time algorithm to find the median of all \(2n\) elements in arrays \(X\) and \(Y\).</p>

  <p><strong>Exercise 9.3-9</strong></p>
  <p>Professor Olay is consulting for an oil company, which is planning a large pipeline running east to west through an oil field of \(n\) wells. The company wants to connect a spur pipeline from each well directly to the main pipeline along a shortest route (either north or south), as shown in Figure 9.2. Given the \(x\)- and \(y\)-coordinates of the wells, how should the professor pick the optimal location of the main pipeline, which would be the one that minimizes the total length of the spurs? Show how to determine the optimal location in linear time.</p>
  <p><em>Answer:</em> Find median of \(y\)-coordinates.</p>

</body>
</html>































